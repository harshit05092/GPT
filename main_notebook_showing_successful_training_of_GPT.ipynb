{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDzHI3/6BeXjZlSto3zx0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit05092/GPT/blob/main/main_notebook_showing_successful_training_of_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNYV1Ekq9Nuy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    \"vocab_size\": 50257, # No of vocab in GPT's BPE\n",
        "    \"context_length\": 256, # The previous context which our gpt will consider\n",
        "    \"emb_dim\": 768, # Number of dimensions that each token will be embedded in\n",
        "    \"n_heads\": 12,  # No of attention heads\n",
        "    \"n_layers\": 12, # No of layers\n",
        "    \"drop_rate\": 0.1, # rate of dropping nn randomly\n",
        "    \"qkv_bias\": False # Bias vector for query, key, value is False\n",
        "}"
      ],
      "metadata": {
        "id": "yV7awKq7XlKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Multiheadattention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False) -> None:\n",
        "    super().__init__() # Allow the inherited class paramenters to be initialized like backward pass and _parameters\n",
        "    assert(d_out % num_heads == 0), \\\n",
        "    \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out # Output dimensions that our transformer will output the code in\n",
        "    self.num_heads = num_heads # Number of attention heads in our multihead attention layer\n",
        "    self.head_dim = d_out // num_heads # Number of dimension each vector will be when then are output by each attention module\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias) # Weight matrix for query vector. din and dout are same generally\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias) # Weight matrix for key vectors\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias) # Weight matrix for value vectors\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout) # Dropout rate to prevent overfitting. For larger models it is set to 0.1\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    query = self.W_query(x)  # Matrix multiplication making query vector\n",
        "    key = self.W_key(x) # Matrix multiplication for making key vector\n",
        "    value = self.W_value(x) # Matrix multiplication for making value vector\n",
        "    key = key.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    query = query.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    value = value.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    key = key.transpose(1, 2)\n",
        "    value = value.transpose(1, 2)\n",
        "    query = query.transpose(1, 2)\n",
        "    attn_score = query @ key.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_score.masked_fill_(mask_bool, -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_score / key.shape[-1] ** 0.5, dim = -1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ value).transpose(1, 2)\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "yrkFquQapbTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModule(nn.Module):\n",
        "\n",
        "  def __init__(self, cfg) -> None:\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # Creates token embedding matrix of size vocab_size * emb_dim\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # Creates token embedding matrix of size context_length * emb_dim\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]) # Sets Dropout to drop_rate\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg)\n",
        "        for _ in range(cfg[\"n_layers\"])]\n",
        "    )# Generates a list of transformer according to the number of layers.\n",
        "     # If 12 then generate list to 12 transformer block stacked on each other\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg) # Set nomalization equal to layer normalization which is usually done in language models\n",
        "\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    ) # Final projection layer that turns each model embeddings into vocabulary logits for next token prediction\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    token_embeds = self.token_emb(in_idx) # Gives token embeddingds in shape of in_idx * emb_dim\n",
        "    pos_embeds = self.pos_emb(\n",
        "        torch.arange(seq_len, device=in_idx.device)\n",
        "        ) # Creates one-D tensor of size seq_len that represents absolute positioin in the sequence and ensures that it is placed into the\n",
        "          # right CPU/GPU\n",
        "    x = token_embeds + pos_embeds # Gives the postional token embeddings\n",
        "    x = self.drop_emb(x) # Drops the 10%(0.1) of the input to prevent overfitting\n",
        "    x = self.trf_blocks(x) # Passes them through transformer block\n",
        "    x = self.final_norm(x) # Passes them through layer normalization\n",
        "    logits = self.out_head(x) # Gives the next prediction\n",
        "    return logits"
      ],
      "metadata": {
        "id": "QPnftNXRbcoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module): # TransformerBlock class is a sub-block of nn.Module\n",
        "\n",
        "  def __init__(self, cfg) -> None:\n",
        "    super().__init__() # Initializes parameters from nn.Module\n",
        "    self.attn = Multiheadattention(\n",
        "        cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"context_length\"], cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg[\"qkv_bias\"]\n",
        "    )# Sets the attention module to multiheadattention\n",
        "    self.ff = FeedForward(cfg) # first projects the emb dim into 4 * emb_dim and then applies GeLU activation and then\n",
        "                               # converts 4 * emb_dim into emb_dim\n",
        "    self.norm1 = LayerNorm(cfg) # Applies layer normalization\n",
        "    self.norm2 = LayerNorm(cfg) # Applies layer normalization\n",
        "    self.dropout = nn.Dropout(cfg[\"drop_rate\"]) # Sets dropout rate to 0.1\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x # Shortcut connection for attention blocks\n",
        "    x = self.norm1(x) # Applies layer normalization\n",
        "    x = self.attn(x) # Applies multi-head attention\n",
        "    x = self.dropout(x) # Applies dropout rate\n",
        "    x = x + shortcut # Applies shortcut connection\n",
        "\n",
        "    shortcut = x # Shorcut connection for FeedForward in which GeLU activation is applied\n",
        "    x = self.norm2(x) # Applies layer normalization\n",
        "    x = self.ff(x) # first projects the emb dim into 4 * emb_dim and then applies GeLU activation and then\n",
        "                   # converts 4 * emb_dim into emb_dim\n",
        "    x = self.dropout(x) # Applies dropout rate\n",
        "    x = x + shortcut # Applies shortcut connection\n",
        "    return x"
      ],
      "metadata": {
        "id": "C8ERYt1N_NmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, cfg) -> None:\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(cfg[\"emb_dim\"])) # Trainable parameter gamma\n",
        "    self.shift = nn.Parameter(torch.zeros(cfg[\"emb_dim\"])) # Trainable parameter beta\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True) # Calculating mean of the layer\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False) # Calculating variance of the layer\n",
        "    x = (x - mean) / torch.sqrt(var + 1e-5) # calculating standardized number. Dividing by 1e-5 to prevent diving by 0\n",
        "    return x * self.scale + self.shift # Returning x * gamma + beta"
      ],
      "metadata": {
        "id": "pdOXFq83_GK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module): # Calculates gaussian expression linear unit(GeLU)\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "VDuA_5YgJXY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg) -> None:\n",
        "      super().__init__()\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "          GELU(),\n",
        "          nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "      ) # Projects the embedding dimension into embedding_dimension * 4 and then applies GeLU activation and then gives\n",
        "        # the output in original emb_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "HhAqeeXTghIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModule(cfg)\n",
        "batch = torch.tensor([\n",
        "    [6109, 3626, 6100, 345],\n",
        "    [6109, 1110, 6622, 257]\n",
        "])\n",
        "out = model(batch)\n",
        "print(\"Input batch: \\n\", batch)\n",
        "print(\"Output batch: \\n\", out)"
      ],
      "metadata": {
        "id": "PXRjh0XRI8oC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8c2842-1797-4ea7-bc50-2ed48ccb9f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch: \n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "Output batch: \n",
            " tensor([[[-1.7955e-01,  2.8515e-01, -7.6131e-01,  ..., -4.8374e-01,\n",
            "          -4.2503e-01, -1.7187e-01],\n",
            "         [-6.2581e-01, -3.7480e-01, -9.7020e-01,  ...,  1.9168e-01,\n",
            "          -1.3234e+00, -2.7643e-01],\n",
            "         [ 5.1744e-01,  1.3866e-01,  2.4886e-01,  ...,  3.5054e-01,\n",
            "          -7.7531e-02, -8.0042e-02],\n",
            "         [-2.5664e-01, -6.9693e-01, -9.9479e-01,  ..., -4.4732e-02,\n",
            "           6.1773e-02,  1.3467e-01]],\n",
            "\n",
            "        [[-2.2379e-01,  1.1651e-01, -9.9836e-01,  ..., -1.5730e-01,\n",
            "          -4.4800e-01, -2.8646e-02],\n",
            "         [-8.7227e-01, -3.9389e-01, -1.1099e+00,  ...,  3.3035e-01,\n",
            "          -9.2395e-02, -1.8597e-05],\n",
            "         [ 4.5988e-01, -1.4274e-01, -1.2227e-01,  ...,  2.7490e-01,\n",
            "           5.8297e-02, -8.9931e-02],\n",
            "         [-6.2163e-01, -4.4854e-01, -4.7675e-01,  ..., -3.6519e-01,\n",
            "           3.4402e-01, -3.8154e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_sample(model, idx,\n",
        "                         max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "    logits = logits[:, -1, :]\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "    idx_next = torch.argmax(probas, dim = -1, keepdim=True)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "nLJUhe8GcJJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the determined device\n",
        "model.to(device)\n",
        "\n",
        "def text_to_token(tokenizer, text):\n",
        "  encoded_text = tokenizer.encode(text)\n",
        "  encoded_text = torch.tensor(encoded_text, dtype=torch.long).unsqueeze(0) # Added .unsqueeze(0) to make it 2D\n",
        "  return encoded_text\n",
        "def token_ids_to_text(tokenizer, token_ids):\n",
        "  # If token_ids is 2D (batch_size, seq_len), convert to 1D before decoding\n",
        "  if token_ids.dim() > 1:\n",
        "    token_ids = token_ids.squeeze(0) # Assuming batch_size is 1 for decoding\n",
        "  decoded_text = tokenizer.decode(token_ids.tolist())\n",
        "  return decoded_text\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Ensure idx is on the correct device\n",
        "initial_idx = text_to_token(tokenizer, start_context).to(device)\n",
        "\n",
        "token_ids = generate_text_sample(\n",
        "    model=model,\n",
        "    idx = initial_idx,\n",
        "    max_new_tokens=5,\n",
        "    context_size=256\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(tokenizer, token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxOGcURFh5EP",
        "outputId": "c49ceb67-1277-45cf-ac38-56a4d1433caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you?\"\n",
            "\"Oh,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(\n",
        "    [[16833, 3626, 6100], # \"every effort moves\"\n",
        "    [40,    1107, 588]] #\"i really like\"\n",
        ").to(device)\n",
        "target = torch.tensor(\n",
        "    [[3626, 6100, 345], # \"every effort moves you\"\n",
        "    [1107, 588, 11311]] # \"really like chocolate\"\n",
        ").to(device)\n",
        "with torch.no_grad():\n",
        "  logits = model(input)\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(probas.shape)\n",
        "token_ids = torch.argmax(probas, dim = -1, keepdim=True)\n",
        "print(token_ids)\n",
        "print(f\"Target batch 1: {token_ids_to_text(tokenizer, target[0])}\")\n",
        "print(f\"Output batch 1: {token_ids_to_text(tokenizer, token_ids[0].flatten())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR_Rh6Yr7rmr",
        "outputId": "4c84ec13-cfba-4ae2-b529-18594e89a6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n",
            "tensor([[[ 832],\n",
            "         [ 550],\n",
            "         [4252]],\n",
            "\n",
            "        [[ 367],\n",
            "         [ 262],\n",
            "         [ 757]]], device='cuda:0')\n",
            "Target batch 1:  effort moves you\n",
            "Output batch 1:  through had sun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], target[text_idx]]\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], target[text_idx]]\n",
        "print(\"Target 1 probabilites:\", target_probas_1)\n",
        "print(\"Target 2 probabilites:\", target_probas_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJdSheJd99Hj",
        "outputId": "df3c260c-539e-4860-dd0d-7bad0e8bd156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target 1 probabilites: tensor([1.0411e-04, 2.6049e-05, 1.0385e-05], device='cuda:0')\n",
            "Target 2 probabilites: tensor([1.6884e-05, 5.1877e-05, 4.9942e-06], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(\"Log probabilites:\", log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugv9MpDXAlA7",
        "outputId": "0d152d3d-8f59-428b-98d2-ed3733641d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log probabilites: tensor([ -9.1700, -10.5555, -11.4751, -10.9891,  -9.8666, -12.2072],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(\"Average log probabilites:\", avg_log_probas)\n",
        "neg_avg_log_probas = -torch.mean(log_probas)\n",
        "print(\"Negative average log probabilites:\", neg_avg_log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olmcnN1cBCxH",
        "outputId": "00ebea34-41e1-4b98-b38d-9e68575d0b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average log probabilites: tensor(-10.7106, device='cuda:0')\n",
            "Negative average log probabilites: tensor(10.7106, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_flat = logits.flatten(0, 1)\n",
        "target_flat = target.flatten()\n",
        "print(\"Logits shape:\", logits_flat.shape)\n",
        "print(\"Target shape:\", target_flat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FefzeNIhBOfd",
        "outputId": "bda66b7b-0570-403e-ccfd-cd43b953a9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([6, 50257])\n",
            "Target shape: torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, target_flat)\n",
        "perplexity = torch.exp(loss)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Perplexity:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_LthXscB3uX",
        "outputId": "5aac16f4-79de-4d35-8b9a-a96b0973eac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(10.7106, device='cuda:0')\n",
            "Perplexity: tensor(44828.9961, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3edff065",
        "outputId": "741a6c36-048f-47da-e7c2-900ff93530c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/the-verdict.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "  text = file.read()\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTH5qBn7B8r-",
        "outputId": "f4456055-c0dd-42ea-d8e8-18a0cf63517c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "J09wFvzzFobG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "UDdqqLIWEF6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(len(text) * train_ratio)\n",
        "train_text = text[:split_idx]\n",
        "val_text = text[split_idx:]"
      ],
      "metadata": {
        "id": "a6kBKHGAFm4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader_v1(\n",
        "    train_text,\n",
        "    batch_size = 2,\n",
        "    max_length = 256,\n",
        "    stride = cfg[\"context_length\"],\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    num_workers = 0\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_text,\n",
        "    batch_size = 2,\n",
        "    max_length = 256,\n",
        "    stride = cfg[\"context_length\"],\n",
        "    shuffle = False,\n",
        "    drop_last = True,\n",
        "    num_workers = 0\n",
        ")\n",
        "print(\"Train loader:\", train_loader)\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "print(\"Val loader:\", val_loader)\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S3Fbn2jGJws",
        "outputId": "50f19e3a-4fb8-4cd8-e74b-5e02d2f96487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader: <torch.utils.data.dataloader.DataLoader object at 0x7e0027f70830>\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "Val loader: <torch.utils.data.dataloader.DataLoader object at 0x7e0027f50410>\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "PRugfvjrG81J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    # Use PyTorch 2.9 or newer for stable mps results\n",
        "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
        "    if (major, minor) >= (2, 9):\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnGRDnzEIdJj",
        "outputId": "75060ab9-f363-4487-c425-6d9feb9e7f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device.\n",
            "Training loss: 10.987756623162163\n",
            "Validation loss: 10.98848819732666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    # Fix: Pass tokenizer first, then start_context\n",
        "    encoded = text_to_token(tokenizer, start_context).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_sample(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(tokenizer, token_ids)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "X7lCI96YJIZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModule(cfg)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"I am\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jAnD82CJYyd",
        "outputId": "157058a9-34ca-4a22-cf51-d35b4d3a5302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.818, Val loss 9.930\n",
            "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.336\n",
            "I am,,,,,,,,,,,,,,.                                   \n",
            "Ep 2 (Step 000010): Train loss 6.623, Val loss 7.053\n",
            "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.605\n",
            "I am,,,,,, and,,,,,,, and,.                                 \n",
            "Ep 3 (Step 000020): Train loss 5.532, Val loss 6.507\n",
            "Ep 3 (Step 000025): Train loss 5.399, Val loss 6.389\n",
            "I am, and, and, and the to the of the to the to the.                                  \n",
            "Ep 4 (Step 000030): Train loss 4.895, Val loss 6.280\n",
            "Ep 4 (Step 000035): Train loss 4.648, Val loss 6.304\n",
            "I am a a--as of the picture the of the picture--as of the picture.             \"I\"I had the  \"I the picture to the picture and I had been.\n",
            "Ep 5 (Step 000040): Train loss 4.023, Val loss 6.165\n",
            "I am he had been the \"Oh, and he had been the fact of the last I felt to me--I had been.                        \n",
            "Ep 6 (Step 000045): Train loss 3.625, Val loss 6.172\n",
            "Ep 6 (Step 000050): Train loss 3.045, Val loss 6.144\n",
            "I am, the course I was his a little the.  \"I had the last word.          \"I turned, and I had a little.   \"I looked, and I had a\n",
            "Ep 7 (Step 000055): Train loss 2.948, Val loss 6.183\n",
            "Ep 7 (Step 000060): Train loss 2.230, Val loss 6.128\n",
            "I am he had been through, and in the picture--I felt, and Mrs.  \"I looked, I felt to me to have to see a smile behind his pictures--I looked up at the honour of the donkey, and were, and\n",
            "Ep 8 (Step 000065): Train loss 1.774, Val loss 6.162\n",
            "Ep 8 (Step 000070): Train loss 1.475, Val loss 6.229\n",
            "I am he had been through, and in spite of, and he had been through, in a so when he had been through!  \"Oh, in the moment--as Jack himself, as once one had been the donkey. \"There were days\n",
            "Ep 9 (Step 000075): Train loss 1.135, Val loss 6.268\n",
            "Ep 9 (Step 000080): Train loss 0.858, Val loss 6.298\n",
            "I am his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\"\n",
            "Ep 10 (Step 000085): Train loss 0.627, Val loss 6.382\n",
            "I amAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the moment--as Jack himself, he had again run over from Monte Carlo; and Mrs.\n",
            "Training completed in 0.59 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l4pjWeARJ9Wj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}